{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1GNoSZnVOVk9K7Jgw8OMMgGz03IWKEg14",
      "authorship_tag": "ABX9TyMMgIk75Rsm4HcQHB6oS+Vj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Through GANs, it has become very easy to automatically replace real face of a person in a video by a synthetic face. There have been many scandals which involve the faces of male or female journalists appear in pornography. If this can happen with the leading journalists then it can also happen with a normal person and thats a matter of worry for all of us. People are making huge use of social media platforms to upload thir videos on instagram and facebook. These videos can be used to train GANs that will generate deepfake videos (videos having original faces removed and fake faces put up there). That means that almost anyone can be the target of a malicious attack by a deepfake video in which they would appear to be talking or behaving in a way that they did not.\n",
        "\n",
        "# Now, kidnappers and blackmailers are taking advantage of this technology and coming up with online scams that targets normal people and celebraties. A professional actor is hired by kidnappers and blackmailers who is paid to act in a recorded video to say bad things. Then, criminals train a GAN using images of a person who needs to be blackmailed or kidnapped, by fetching his/her videos from the social media. Using trained GAN, the blackmailers replace the face of the actor in their prerecorded video with the person's face who needs to be blackmailed. Then, they blackmail the target with this deepfake video, threatening to send it to relatives, neighbors, and other peers.\n",
        "\n",
        "# Suppose you are a forensic consultant hired to develop an algorithm that can detect whether video is deepfake or not and doesn't suck up lot of computational resources and it should be fast and hence runs smoothly with or without the presence of GPUs. \n",
        "\n",
        "# Let's put things in perspective that which algorithm we can use ? But in order to answer this question, first we have to look at how we are going to convert our videos into the dataset. \n",
        "\n",
        "# Well, first we have to read each frame of each video, whether deepfake or real and find out bounding boxes of faces in each frame and then allign (rotate) those faces properly so that for each face of each frame, the location of eyes, nose, cheeks is same and then extract features from all the alligned faces of each frame of deepgake and real videos and it will give us a fixed dimensional feature vector for each of the faces and their labels, real or fake which will convert our dataset of videos into a tabular dataset having number of rows equal to the number of frames in all the deepfakes as well as real videos with real or fake label in front of each of them. \n",
        "\n",
        "# We can make a guess that the number of fixed dimensional feature vectors extracted from each alligned face images can be huge and on the top of that, we will be having lot of frames and hence the dataset is huge and can have large number of features. We know that in such kind of scenario where the number of features can be huge or the dimensionality of the dataset will be huge, and we want an algorithm which is fast, then the obvious choice is SVM so we will be using support vector machines to perform classification for whether the video is a deepfake or real video. "
      ],
      "metadata": {
        "id": "zGaz0vGC3gSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make sure you are navigated inside Google drive. "
      ],
      "metadata": {
        "id": "VzNpHSGx0ju8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEYLk3_lnR0"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the fake videos dataset from the following link:\n",
        "# https://zenodo.org/record/4068245/files/DeepfakeTIMIT.tar.gz?download=1\n",
        "\n",
        "# To read more about the dataset, you can goto the following link: \n",
        "# https://www.idiap.ch/en/dataset/deepfaketimit\n",
        "\n",
        "# Download the corrsponding real videos dataset from the following link: \n",
        "# https://conradsanderson.id.au/vidtimit/#downloads\n",
        "\n",
        "# To read more about the dataset, you can navigate to the following link: \n",
        "# http://conradsanderson.id.au/vidtimit/\n",
        "\n"
      ],
      "metadata": {
        "id": "1nDYk5Xm1io8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HOKXDY0tai0"
      },
      "source": [
        "! tar -xvf /content/drive/MyDrive/DeepfakeTIMIT.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n61jHONDuUAl"
      },
      "source": [
        "! unzip /content/drive/MyDrive/VidTIMIT.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the video files from the above zip folders with the help of above statements. "
      ],
      "metadata": {
        "id": "6k4NTYpm3Ywa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4uxCkp3unp-"
      },
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjWH8r4gZhTs"
      },
      "source": [
        "os.mkdir(path=\"./DeepfakeTIMIT_frames\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJpdsynXUCvz"
      },
      "source": [
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill the function below to read the frames of each video (deepfake or real) to a folder. "
      ],
      "metadata": {
        "id": "Z8GXtpaJFn6X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFtR7EpSUla2"
      },
      "source": [
        "def process_vid_to_frames(path_to_write_frames,vid):\n",
        "\n",
        "  vidcapture = cv2.VideoCapture(vid)\n",
        "\n",
        "  if vidcapture.isOpened() == False:\n",
        "    print(\"Error opening file\")\n",
        "  else:\n",
        "    vid_fps = vidcapture.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frame Rate of the Video is\",vid_fps)\n",
        "\n",
        "  frame_counter = 0\n",
        "\n",
        "  while vidcapture.isOpened():\n",
        "\n",
        "    ret,frame = vidcapture.read()\n",
        "\n",
        "    if ret == True:\n",
        "      cv2.imwrite(os.path.join(path_to_write_frames,vid.split(\"/\")[-1]+\"_frame\"+str(frame_counter)+\".jpg\"),frame)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    frame_counter = frame_counter + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The function below creates a folder with the name of the video file for which you want to write each and individual frames"
      ],
      "metadata": {
        "id": "SdQvXYs1F8NR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSx29c5HIHaR"
      },
      "source": [
        "def process_video_per_folder(path_to_write_frames,vid):\n",
        "\n",
        "  folder_path_to_write_frames = os.path.join(path_to_write_frames,vid.split(\"/\")[-1])\n",
        "  os.mkdir(folder_path_to_write_frames)\n",
        "\n",
        "  process_vid_to_frames(folder_path_to_write_frames,vid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The function below creates a root directory inside which the directories of deepfake or real videos of individual persons are created and frames are read from each video and written inside this directory. This process will happen in parallel as show below that the function is using concurrent.futures"
      ],
      "metadata": {
        "id": "tOxpRXNmGJw4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-e_R7BfeBRH"
      },
      "source": [
        "def process_folder_vids(dst_base_path,src_base_path,src_folder):\n",
        "\n",
        "  src_folder_path = os.path.join(src_base_path,src_folder)\n",
        "\n",
        "  list_of_vids = list(glob(pathname=os.path.join(src_folder_path+\"/\",\"*.avi\")))\n",
        "\n",
        "  path_to_write_frames = os.path.join(dst_base_path,list_of_vids[0].split(\"/\")[-2])\n",
        "  os.mkdir(path_to_write_frames)\n",
        "  paths_to_write_frames = [path_to_write_frames]*len(list_of_vids)\n",
        "\n",
        "  with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "\n",
        "    executor.map(process_video_per_folder,paths_to_write_frames,list_of_vids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24SksyLlmLh-"
      },
      "source": [
        "def each_folder_vids(dst_base_path,src_base_path,src_folder):\n",
        "\n",
        "  process_folder_vids(dst_base_path,src_base_path,src_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function below process all deepfake and real videos of all persons into their respective frames and write them into the destination directories. "
      ],
      "metadata": {
        "id": "a21CSXTOHJ0q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I9hZsvDtPlI"
      },
      "source": [
        "def process_all_folders(dst_base_path,src_base_path):\n",
        "\n",
        "  list_of_folders = os.listdir(path=src_base_path)\n",
        "  dst_base_paths = [dst_base_path]*len(list_of_folders)\n",
        "  src_base_paths = [src_base_path]*len(list_of_folders)\n",
        "\n",
        "  with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "\n",
        "    executor.map(each_folder_vids,dst_base_paths,src_base_paths,list_of_folders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhahaXvQ1vCI"
      },
      "source": [
        "process_all_folders(dst_base_path=\"/content/drive/MyDrive/DeepfakeTIMIT_frames\",\n",
        "                    src_base_path=\"/content/drive/MyDrive/DeepfakeTIMIT/higher_quality\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rce27RPJOLGb"
      },
      "source": [
        "os.mkdir(path=\"/content/drive/MyDrive/VidTIMIT_frames\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtPrcSfQjU_"
      },
      "source": [
        "process_all_folders(dst_base_path=\"/content/drive/MyDrive/VidTIMIT_frames\",\n",
        "                    src_base_path=\"/content/drive/MyDrive/VidTIMIT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In order to extract faces and their keypoints (locations of eyes, nose and cheeks) from each frame of each video (deepfake or real) ,we will be using a deep learning based approch called MTCNNs (Multi-task cascaded Convolutional Neural Networks), therefore we will be installing a library called mtcnn where we have this pretrained CNN to perform face extraction and keypoint detection for us. So, let's install this library. \n",
        "\n",
        "# You can read more about MTCNN here: \n",
        "# https://arxiv.org/abs/1604.02878"
      ],
      "metadata": {
        "id": "uB8UXLYcHb78"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v50Anx7mRCmH"
      },
      "source": [
        "! pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK9hJqWDUwqR"
      },
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFZhagBDU2Kj"
      },
      "source": [
        "img = cv2.cvtColor(cv2.imread(\"/content/drive/MyDrive/VidTIMIT_frames/fadg0/sa1.avi/sa1.avi_frame0.jpg\"),\n",
        "                   cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's perform face extraction and keypoint detection on face using MTCNN. \n",
        "\n",
        "# To know more about MTCNN library and how to use it, you can read here: \n",
        "# https://github.com/ipazc/mtcnn"
      ],
      "metadata": {
        "id": "mRq7u6O4ITiB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju5_zTo_YZHf"
      },
      "source": [
        "detector = MTCNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiKfLFvlYbK_"
      },
      "source": [
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWpPOrdnYegR"
      },
      "source": [
        "result = detector.detect_faces(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoHEFv2sY7ax"
      },
      "source": [
        "bounding_box = result[0]['box']\n",
        "keypoints = result[0]['keypoints']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17zZwQf8ZNab"
      },
      "source": [
        "cv2.rectangle(img,\n",
        "              (bounding_box[0], bounding_box[1]),\n",
        "              (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n",
        "              (0,255,0),\n",
        "              2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnSlg6glaEGZ"
      },
      "source": [
        "cv2.circle(img,(keypoints['left_eye']), 2, (0,255,0), 2)\n",
        "cv2.circle(img,(keypoints['right_eye']), 2, (0,255,0), 2)\n",
        "cv2.circle(img,(keypoints['nose']), 2, (0,255,0), 2)\n",
        "cv2.circle(img,(keypoints['mouth_left']), 2, (0,255,0), 2)\n",
        "cv2.circle(img,(keypoints['mouth_right']), 2, (0,255,0), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJ5GSY-aOPO"
      },
      "source": [
        "cv2.imwrite(\"sample.jpg\", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zteZ1f4aY3j"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAhvMglmagZn"
      },
      "source": [
        "sample_img = cv2.imread(\"/content/drive/MyDrive/sample.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# As you can see below that MTCNN has extracted a bounding box from the face inside the image and also detected keypoints of left eye, right eye, nose, left cheek and right cheek. "
      ],
      "metadata": {
        "id": "f4bNuAH7IdY2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sbw6reXapC9"
      },
      "source": [
        "plt.imshow(sample_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In order to allign all the faces in the frames of a video,we have to first extract the angle of eyes with horizontal becauase in all the frames, face will moving in all the directions and hence we have to allign (rotate) all the faces at 0 degree with the horizontal or at some fix angle with the horizontal and for this we have to determine the angle of the face with the horizontal which will be actually equal to angle of the eyes made with the horizontal. "
      ],
      "metadata": {
        "id": "Sgy56vdjIztK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FJPGvNXasM-"
      },
      "source": [
        "def determining_angle_of_eyes(left_eye_keypoint,right_eye_keypoint):\n",
        "\n",
        "  del_y = right_eye_keypoint[1] - left_eye_keypoint[1]\n",
        "  del_x = right_eye_keypoint[0] - left_eye_keypoint[0]\n",
        "\n",
        "  return np.degrees(np.arctan2(del_y,del_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Since, we are fetching faces from each of the frames in the videos, we have to also rescale each of the fetched faces to the fixed size as we want to convert the fetched faces and their keypoints to the fixed dimensional feature vector and that can only be possible when each fetched face is converted into a fixed size image (rescaling). So, below function does the rescaling of fetched faces. Please exmaine this funcmtion carefully and see what is happening here because such kind of operations are used a lot while fetching features from the images. "
      ],
      "metadata": {
        "id": "HlkyQDEtMr6P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk0uXjsm1BEE"
      },
      "source": [
        "def determine_scaling_factor(org_img_left_eye_kp,org_img_right_eye_kp,rescaled_img_left_eye_kp,\n",
        "                             rescaled_img_right_eye_kp):\n",
        "  \n",
        "  del_y_org = org_img_right_eye_kp[1] - org_img_left_eye_kp[1]\n",
        "  del_x_org = org_img_right_eye_kp[0] - org_img_left_eye_kp[0]\n",
        "\n",
        "  dist_org = np.sqrt((del_y_org**2) + (del_x_org**2))\n",
        "\n",
        "  del_y_rescaled = rescaled_img_right_eye_kp[1] - rescaled_img_left_eye_kp[1]\n",
        "  del_x_rescaled = rescaled_img_right_eye_kp[0] - rescaled_img_left_eye_kp[0]\n",
        "\n",
        "  dist_rescaled = np.sqrt((del_y_rescaled)**2 + (del_x_rescaled)**2)\n",
        "\n",
        "  scaling_factor = dist_rescaled/dist_org\n",
        "\n",
        "  return scaling_factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The function shown below is the main function responsible for extracting faces as well as rescaling and reallinging all the faces and will be calling the above functions indirectly. \n",
        "\n",
        "# To know more about cv2.getRotationMtrix2D and cv2.warpAffine tht what they are doing, you can read following: \n",
        "\n",
        "# https://learnopencv.com/tag/cv2-getrotationmatrix2d/\n",
        "\n",
        "# https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html"
      ],
      "metadata": {
        "id": "lb3ltVr7Ntti"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYgOHq7ejZH1"
      },
      "source": [
        "def crop_faces_and_reallign(rescaled_img_width,eye_loc_percentage,org_img,left_eye_kp,right_eye_kp):\n",
        "\n",
        "  rescaled_img_left_eye_kp = (eye_loc_percentage[0]*rescaled_img_width, eye_loc_percentage[1]*rescaled_img_width)\n",
        "  rescaled_img_right_eye_kp = ((1-eye_loc_percentage[0])*rescaled_img_width, eye_loc_percentage[1]*rescaled_img_width)\n",
        "  #print(rescaled_img_left_eye_kp,rescaled_img_right_eye_kp)\n",
        "\n",
        "  eyes_center = ((left_eye_kp[0] + right_eye_kp[0])//2, (left_eye_kp[1] + right_eye_kp[1])//2)\n",
        "  #print(eyes_center)\n",
        "\n",
        "  scaling_factor = determine_scaling_factor(left_eye_kp,right_eye_kp,rescaled_img_left_eye_kp,\n",
        "                                            rescaled_img_right_eye_kp)\n",
        "  #print(scaling_factor)\n",
        "  \n",
        "  eyes_angle = determining_angle_of_eyes(left_eye_kp,right_eye_kp)\n",
        "  #print(eyes_angle)\n",
        "\n",
        "  rotation_matrix = cv2.getRotationMatrix2D(center=eyes_center,angle=eyes_angle,scale=scaling_factor)\n",
        "  #print(rotation_matrix)\n",
        "\n",
        "  #print(rescaled_img_width,rescaled_img_left_eye_kp[1])\n",
        "\n",
        "  translated_x = rescaled_img_width * 0.5\n",
        "  translated_y = rescaled_img_left_eye_kp[1]\n",
        "  #print(rescaled_img_width,rescaled_img_left_kp[1])\n",
        "\n",
        "  rotation_matrix[0,2] = rotation_matrix[0,2] + (translated_x - eyes_center[0])\n",
        "  rotation_matrix[1,2] = rotation_matrix[1,2] + (translated_y - eyes_center[1])\n",
        "\n",
        "  rotated_and_scaled_image = cv2.warpAffine(src=org_img,M=rotation_matrix,dsize=(rescaled_img_width,rescaled_img_width))\n",
        "  \n",
        "  #print(rotated_and_scaled_image)\n",
        "\n",
        "  return rotated_and_scaled_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below function detect single face from single frame of a video. "
      ],
      "metadata": {
        "id": "0GZjF-eBOEK2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85oHnqla_lGj"
      },
      "source": [
        "def detect_single_image_face(org_img,eye_loc_percentage,rescaled_img_width):\n",
        "\n",
        "  rgb_img = cv2.cvtColor(org_img,cv2.COLOR_BGR2RGB)\n",
        "  result = detector.detect_faces(rgb_img)\n",
        "  #print(result)\n",
        "\n",
        "  left_eye_kp = result[0][\"keypoints\"][\"left_eye\"]\n",
        "  right_eye_kp = result[0][\"keypoints\"][\"right_eye\"]\n",
        "\n",
        "  #print(rescaled_img_width,eye_loc_percentage,left_eye_kp,right_eye_kp)\n",
        "\n",
        "  rotated_scaled_face_img = crop_faces_and_reallign(rescaled_img_width,eye_loc_percentage,org_img,left_eye_kp,right_eye_kp)\n",
        "\n",
        "  return rotated_scaled_face_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below function extracts faces from the frames of a single video and write the rescaled (warped) and realligned faces to the folders. "
      ],
      "metadata": {
        "id": "CXvtFE4VOLDB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irl8T9F6bv7x"
      },
      "source": [
        "def extract_single_vid_face_frames_and_save(src_path_to_single_vid_frames,dst_base_path_to_single_person_vids):\n",
        "\n",
        "  #print(src_path_to_single_vid_frames)\n",
        "\n",
        "  single_vid_frames = glob(src_path_to_single_vid_frames+\"/*.jpg\")\n",
        "\n",
        "  #print(single_vid_frames)\n",
        "\n",
        "  os.mkdir(os.path.join(dst_base_path_to_single_person_vids,src_path_to_single_vid_frames.split(\"/\")[-1]))\n",
        "\n",
        "  dst_base_path_to_single_vid = os.path.join(dst_base_path_to_single_person_vids,src_path_to_single_vid_frames.split(\"/\")[-1])\n",
        "\n",
        "  #print(dst_base_path_to_single_vid)\n",
        "  f_counter = 0\n",
        "\n",
        "  for single_img in single_vid_frames:\n",
        "\n",
        "    org_img = plt.imread(single_img)\n",
        "    #print(org_img.shape)\n",
        "    rotated_scaled_face_img = detect_single_image_face(org_img,eye_loc_percentage=(0.375,0.375),rescaled_img_width=224)\n",
        "    #print(rotated_scaled_face_img.shape)\n",
        "    print(\"Extracted face from {}\".format(single_img))\n",
        "\n",
        "    cv2.imwrite(os.path.join(dst_base_path_to_single_vid,single_img.split(\"/\")[-1].split(\".\")[0]+\"_face\"+str(f_counter)+\".jpg\"),rotated_scaled_face_img)\n",
        "    print(\"Written the Image of face into {}\".format(dst_base_path_to_single_vid))\n",
        "    f_counter = f_counter + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The function below call the function above over all the frames of all the videos of a single person in parallel so that extractions and reallignment happens fast. "
      ],
      "metadata": {
        "id": "U6Zs6xpKOZY4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTvjujuqjrgc"
      },
      "source": [
        "def extract_single_person_vids_frames_and_save(src_path_to_single_person_vids,dst_base_path):\n",
        "\n",
        "  #print(src_path_to_single_person_vids)\n",
        "\n",
        "  src_base_path_to_single_person_vids = glob(src_path_to_single_person_vids+\"/*\")\n",
        "  #print(src_base_path_to_single_person_vids)\n",
        "  single_person_vids = list(src_base_path_to_single_person_vids)\n",
        "\n",
        "  #print(single_person_vids)\n",
        "\n",
        "  os.mkdir(os.path.join(dst_base_path,src_path_to_single_person_vids.split(\"/\")[-1]))\n",
        "\n",
        "  dst_base_path_to_single_person_vids = os.path.join(dst_base_path,src_path_to_single_person_vids.split(\"/\")[-1])\n",
        "\n",
        "  #print(dst_base_path_to_single_person_vids)\n",
        "\n",
        "  dst_base_paths_to_single_person_vids = [dst_base_path_to_single_person_vids]*len(single_person_vids)\n",
        "\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "    executor.map(extract_single_vid_face_frames_and_save,single_person_vids,dst_base_paths_to_single_person_vids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, the above function is aclled on the frames of videos of different persons in parallel as shown below. "
      ],
      "metadata": {
        "id": "PxKRXfPFOzae"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n411sX9hmPkl"
      },
      "source": [
        "src_base_path = \"/content/drive/MyDrive/DeepfakeTIMIT_frames\"\n",
        "src_path_to_persons_vids = glob(src_base_path+\"/*\")\n",
        "persons_vids = list(src_path_to_persons_vids)\n",
        "\n",
        "os.mkdir(os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_faces\"))\n",
        "\n",
        "dst_base_path = os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_faces\")\n",
        "\n",
        "dst_base_paths = [dst_base_path]*len(persons_vids)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "  executor.map(extract_single_person_vids_frames_and_save,persons_vids,dst_base_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarly it is done for the real videos as shown below using the above functions. "
      ],
      "metadata": {
        "id": "ZdaYVSkYO9V0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cElgqtKqnqZr"
      },
      "source": [
        "src_base_path = \"/content/drive/MyDrive/VidTIMIT_frames\"\n",
        "src_path_to_persons_vids = glob(src_base_path+\"/*\")\n",
        "persons_vids = list(src_path_to_persons_vids)\n",
        "\n",
        "os.mkdir(os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_faces\"))\n",
        "\n",
        "dst_base_path = os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_faces\")\n",
        "\n",
        "dst_base_paths = [dst_base_path]*len(persons_vids)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "  executor.map(extract_single_person_vids_frames_and_save,persons_vids,dst_base_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import normalized_root_mse, peak_signal_noise_ratio, structural_similarity"
      ],
      "metadata": {
        "id": "HaxviIvKomSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After extracting faces from each individual frames and from all the deepfake and real videos and realligning them, we will extract feature vectors from each image of extracted faces of same size. What kind of features we will be fetching from each same size image of faces. well, there are many features but we will extracting right now only three features which themselves will be in the form of vectors. We will be extracting: \n",
        "\n",
        "# 1. Normalized Root MSE\n",
        "# 2. PSNR (Peak Signal to Noise Ratio)\n",
        "# 3. Structural Similarity\n",
        "# 4. Intensity Histogram\n",
        "\n",
        "# And finally merging (concatenating) them with each other to create a row vector per single face image and then we will labelling them with \"real\" or \"fake\" depending upon from where we are fetching that face. "
      ],
      "metadata": {
        "id": "d1bzpOznPHgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_feature_vector_on_image(image_path):\n",
        "\n",
        "  img = plt.imread(image_path)\n",
        "  blurred_img = cv2.GaussianBlur(img,(3,3),0.5)\n",
        "\n",
        "  nrmse = normalized_root_mse(img,blurred_img)\n",
        "  psnr = peak_signal_noise_ratio(img,blurred_img,data_range=255)\n",
        "  ssim = structural_similarity(img,blurred_img,multichannel=True,gaussian_weights=True,sigma=1.5,\n",
        "                               use_sample_covariance=False,data_range=255)\n",
        "  hist,bins = np.histogram(img.ravel(),32,[0,255],density=True)\n",
        "\n",
        "  feature_vector = np.concatenate([[nrmse],[psnr],[ssim],hist])\n",
        "\n",
        "  return feature_vector\n"
      ],
      "metadata": {
        "id": "zB6sVFZZqepb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below function will call the above function on all the extracted faces of a single video of a single person and write the extracted features returned by above function on single face image into compressed array format and will write these compressed arrays into a directory. "
      ],
      "metadata": {
        "id": "dFSyIWLhQxni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_single_vid_face_frames_feature_vector_and_save(src_path_to_single_vid_frames,dst_base_path_to_single_person_vids):\n",
        "\n",
        "  #print(src_path_to_single_vid_frames)\n",
        "\n",
        "  single_vid_frames = glob(src_path_to_single_vid_frames+\"/*.jpg\")\n",
        "\n",
        "  #print(single_vid_frames)\n",
        "\n",
        "  os.mkdir(os.path.join(dst_base_path_to_single_person_vids,src_path_to_single_vid_frames.split(\"/\")[-1]))\n",
        "\n",
        "  dst_base_path_to_single_vid = os.path.join(dst_base_path_to_single_person_vids,src_path_to_single_vid_frames.split(\"/\")[-1])\n",
        "\n",
        "  #print(dst_base_path_to_single_vid)\n",
        "  f_counter = 0\n",
        "\n",
        "  for single_img in single_vid_frames:\n",
        "\n",
        "    feature_vector = compute_feature_vector_on_image(single_img)\n",
        "    #print(org_img.shape)\n",
        "    #rotated_scaled_face_img = detect_single_image_face(org_img,eye_loc_percentage=(0.375,0.375),rescaled_img_width=224)\n",
        "    #print(rotated_scaled_face_img.shape)\n",
        "    print(\"Extracted feature vector from {}\".format(single_img))\n",
        "\n",
        "    #cv2.imwrite(os.path.join(dst_base_path_to_single_vid,single_img.split(\"/\")[-1].split(\".\")[0]+\"_face\"+str(f_counter)+\".jpg\"),rotated_scaled_face_img)\n",
        "    np.savez_compressed(os.path.join(dst_base_path_to_single_vid,single_img.split(\"/\")[-1].split(\".\")[0]+\"_face\"+str(f_counter)+\".npz\"),a=feature_vector)\n",
        "    print(\"Written the Feature Vector of face into {}\".format(dst_base_path_to_single_vid))\n",
        "    f_counter = f_counter + 1"
      ],
      "metadata": {
        "id": "MvVil4D1tvyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below function will call the above function over all the videos of a single person in parallel. "
      ],
      "metadata": {
        "id": "zM3ocblyRB3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_single_person_vids_frames_feature_vector_and_save(src_path_to_single_person_vids,dst_base_path):\n",
        "\n",
        "  #print(src_path_to_single_person_vids)\n",
        "\n",
        "  src_base_path_to_single_person_vids = glob(src_path_to_single_person_vids+\"/*\")\n",
        "  #print(src_base_path_to_single_person_vids)\n",
        "  single_person_vids = list(src_base_path_to_single_person_vids)\n",
        "\n",
        "  #print(single_person_vids)\n",
        "\n",
        "  os.mkdir(os.path.join(dst_base_path,src_path_to_single_person_vids.split(\"/\")[-1]))\n",
        "\n",
        "  dst_base_path_to_single_person_vids = os.path.join(dst_base_path,src_path_to_single_person_vids.split(\"/\")[-1])\n",
        "\n",
        "  #print(dst_base_path_to_single_person_vids)\n",
        "\n",
        "  dst_base_paths_to_single_person_vids = [dst_base_path_to_single_person_vids]*len(single_person_vids)\n",
        "\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "    executor.map(extract_single_vid_face_frames_feature_vector_and_save,single_person_vids,dst_base_paths_to_single_person_vids)"
      ],
      "metadata": {
        "id": "gwCAUwmJyfHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below code snippet will call the above function over all the videos of all the persons in parallel. "
      ],
      "metadata": {
        "id": "2-qVO4GhRlfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_base_path = \"/content/drive/MyDrive/DeepfakeTIMIT_frames_faces\"\n",
        "src_path_to_persons_vids = glob(src_base_path+\"/*\")\n",
        "persons_vids = list(src_path_to_persons_vids)\n",
        "\n",
        "os.mkdir(os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_feature_vectors\"))\n",
        "\n",
        "dst_base_path = os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_feature_vectors\")\n",
        "\n",
        "dst_base_paths = [dst_base_path]*len(persons_vids)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "  executor.map(extract_single_person_vids_frames_feature_vector_and_save,persons_vids,dst_base_paths)"
      ],
      "metadata": {
        "id": "HGBtzKp5yx7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below code snippet will eb doing the similar things for the real videos. "
      ],
      "metadata": {
        "id": "p3kaZw0tR5Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_base_path = \"/content/drive/MyDrive/VidTIMIT_frames_faces\"\n",
        "src_path_to_persons_vids = glob(src_base_path+\"/*\")\n",
        "persons_vids = list(src_path_to_persons_vids)\n",
        "\n",
        "os.mkdir(os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_feature_vectors\"))\n",
        "\n",
        "dst_base_path = os.path.join(\"/\".join(src_base_path.split(\"/\")[:-1]),src_base_path.split(\"/\")[-1]+\"_feature_vectors\")\n",
        "\n",
        "dst_base_paths = [dst_base_path]*len(persons_vids)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "  executor.map(extract_single_person_vids_frames_feature_vector_and_save,persons_vids,dst_base_paths)"
      ],
      "metadata": {
        "id": "PIs75w6f0Rhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below coding snippets show the whole procedure involved in coverting the feature vectors of each extracted faces of each frame of each video into a dataframe. And then SVM can be applied on the below resulting dataframe after splitting it into traning, cross validation and testing data. We will train the SVM using training data and check it's performance on cross validation data. So, your job finally is to train SVM on the below training data in the form of Dataframe and then determine the performance metrics of the trained SVM. "
      ],
      "metadata": {
        "id": "vgg_HKRaSGig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_faces_folders = os.listdir(\"/content/drive/MyDrive/VidTIMIT_frames_faces_feature_vectors\")\n",
        "fake_faces_folders = os.listdir(\"/content/drive/MyDrive/DeepfakeTIMIT_frames_faces_feature_vectors\")"
      ],
      "metadata": {
        "id": "kKXgcmvIA2sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_folders = list(set(real_faces_folders).intersection(set(fake_faces_folders)))"
      ],
      "metadata": {
        "id": "CSvmXVdhBCff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_training_folders = common_folders[0:int(0.7*len(common_folders))]"
      ],
      "metadata": {
        "id": "-QOOPgKLBMlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_cv_folders = common_folders[int(0.7*len(common_folders)):int(0.9*len(common_folders))]"
      ],
      "metadata": {
        "id": "d3tF_ZwEDNyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_testing_folders = common_folders[int(0.9*len(common_folders)):]"
      ],
      "metadata": {
        "id": "uihbUMYUDbLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_counter = 0"
      ],
      "metadata": {
        "id": "sr-G8q7BI12G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_npz_and_append(src_path_to_single_frame):\n",
        "\n",
        "  global f_counter\n",
        "  f_counter = f_counter + 1\n",
        "\n",
        "  return np.load(src_path_to_single_frame)['a']"
      ],
      "metadata": {
        "id": "9ECovEtmKpxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_vid_npz_and_append(src_path_to_single_vid_frames):\n",
        "\n",
        "  single_vid_frames = list(glob(src_path_to_single_vid_frames+\"/*.npz\"))\n",
        "\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    return executor.map(load_npz_and_append,single_vid_frames)"
      ],
      "metadata": {
        "id": "OEBjHYL_Ft23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_person_vids_npz_and_append(src_path_to_single_person_vids):\n",
        "\n",
        "  src_base_path_to_single_person_vids = list(glob(src_path_to_single_person_vids+\"/*\"))\n",
        "\n",
        "  with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    return executor.map(load_single_vid_npz_and_append,src_base_path_to_single_person_vids)"
      ],
      "metadata": {
        "id": "Nqk-QLaZMdlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = list()\n",
        "src_base_path = \"/content/drive/MyDrive/DeepfakeTIMIT_frames_faces_feature_vectors\"\n",
        "src_path_to_persons_vids = glob(src_base_path+\"/*\")\n",
        "persons_vids = list(src_path_to_persons_vids)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  features.append(executor.map(load_single_person_vids_npz_and_append,persons_vids))"
      ],
      "metadata": {
        "id": "UiN3WFqTNYJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data_features = list()\n",
        "\n",
        "for obj1 in list(features[0]):\n",
        "\n",
        "  for obj2 in list(obj1):\n",
        "\n",
        "    complete_data_features.extend(list(obj2))"
      ],
      "metadata": {
        "id": "mOz4Z1qCXmlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_training_labels = np.array(['fake']*f_counter)"
      ],
      "metadata": {
        "id": "2Iiy2jYkaM3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_counter = 0"
      ],
      "metadata": {
        "id": "MGjgNkb-mgb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = list()\n",
        "src_base_path = \"/content/drive/MyDrive/VidTIMIT_frames_faces_feature_vectors\"\n",
        "src_path_to_persons_vids = glob(src_base_path+\"/*\")\n",
        "persons_vids = list(src_path_to_persons_vids)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  features.append(executor.map(load_single_person_vids_npz_and_append,persons_vids))"
      ],
      "metadata": {
        "id": "cEYEBx-DaOld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for obj1 in list(features[0]):\n",
        "\n",
        "  for obj2 in list(obj1):\n",
        "\n",
        "    complete_data_features.extend(list(obj2))"
      ],
      "metadata": {
        "id": "4iXTjk1hlJrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(complete_data_features)"
      ],
      "metadata": {
        "id": "HgSTgJMQnHmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_training_labels = np.array(['real']*f_counter)"
      ],
      "metadata": {
        "id": "MvMDGPMunNil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_training_labels.shape"
      ],
      "metadata": {
        "id": "RiSJjE_KoOFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_training_labels.shape"
      ],
      "metadata": {
        "id": "_j6HmFkMoZl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data_features = np.asarray(complete_data_features)"
      ],
      "metadata": {
        "id": "GGlVFiqYofmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data_features.shape"
      ],
      "metadata": {
        "id": "vD3H6WmLtKUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data_features"
      ],
      "metadata": {
        "id": "riw4IGdewqmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data_labels = np.concatenate((fake_training_labels,real_training_labels),axis=0)"
      ],
      "metadata": {
        "id": "aPndnk7JtM4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data_labels = complete_data_labels.reshape(complete_data_labels.shape[0],1)"
      ],
      "metadata": {
        "id": "YgAOLRdfufsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.concatenate((complete_data_features,complete_data_labels),axis=1)"
      ],
      "metadata": {
        "id": "t93S_xWbuh-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "KrSgVMYqu2ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xyMbLRH8z3PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame(data=data)"
      ],
      "metadata": {
        "id": "KKKTeGXa2nKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df"
      ],
      "metadata": {
        "id": "2oKIJ9a02uRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEHgaeis4OmM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}